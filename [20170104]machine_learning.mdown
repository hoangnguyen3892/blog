Title: Basic machine learning 
Date: 2017-01-04 14:56 
Modified: 2017-01-04 14:56 
Category: Machine Learning
Tags: machine learning, grid search, parameter tuning
Slug: my-super-post
Authors: Hoang Nguyen
Summary: Terminal for Dummies


# How to train model effectively
---


> **Steps:**
- Extract features
- Build model
- Select features
- Train model

## Learning algorithm

**1. Regression**

a. Linear regression
- Practice Linear Regression by taking part in the competition: [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
    + Data exploration: [here](https://www.kaggle.com/pmarcelino/house-prices-advanced-regression-techniques/comprehensive-data-exploration-with-python)
    + Data preprocessing and Model building: [here](https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models) or [here](https://www.kaggle.com/miguelangelnieto/house-prices-advanced-regression-techniques/pca-and-regression)

b. Logistic regression

c. Polynomial regression

d. Stepwise regression

e. Ridge regression

f. Lasso regression

g. ElasticNet regression

**2. Naive Bayes**

C = cancer

* P(C) = 0.01
* P(not C) = 0.99

Test: 	

* 90% it is positive if you have C => P(Positive|C) = 0.9, P(Negative|C) = 0.1
* 90% it is negative if you don't have C => P(Negative|not C) = 0.9, P(Positive|notC) = 0.1


**Prior**

P(C) = 0.01


**Posterior**

* P(C|Positive) = P(C) x P(Positive|C) = 0.01 x 0.9 = 0.009
* P(not C|Positive) = P(not C) x P(Positive|not C) = 0.99 x 0.1 = 0.099

**3. Support Vector Machine**

**4. Decision Tree/Ensemble Modeling**

a. Random Forest and bagging

b. Adaboost/Xgboost and boosting

    


![**MIND MAP**](https://s3.amazonaws.com/MLMastery/MachineLearningAlgorithms.png?__s=35aocoess5ybgzt3rkww)

> __**Relating topic artical:**__
- [Approaching (Almost) Any Machine Learning Problem](http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/)
- [A Kaggler's Guide to Model Stacking in Practice](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)
- [KAGGLE ENSEMBLING GUIDE](http://mlwave.com/kaggle-ensembling-guide/)
- [Support Vector Machine – Simplified](https://www.analyticsvidhya.com/blog/2014/10/support-vector-machine-simplified/)
- [What is the difference between bagging and boosting?](http://quantdare.com/2016/04/what-is-the-difference-between-bagging-and-boosting/)
- **Code** [Logistic Regression in Python](http://blog.yhat.com/posts/logistic-regression-and-python.html)
- **Code** [XGBoost](http://www.xavierdupre.fr/app/pymyinstall/helpsphinx/_downloads/example_xgboost.pdf)
- **Code** [Kaggle for the paws](https://andraszsom.wordpress.com/2016/07/27/kaggle-for-the-paws/)
- **Code** [6 Easy Steps to Learn Naive Bayes Algorithm (with code in Python)](https://www.analyticsvidhya.com/blog/2015/09/naive-bayes-explained/)



## Feature selection methods
- [Introduction to Feature Selection methods with an example (or how to select the right variables?)](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/)

**1. Filter methods**

| **Feature/Response**	| **Continuous**			| **Categorical**	| 
| ----------------------| --------------------------| ------------------| 
| Continuous			| Pearson's Correlation 	| LDA				| 
| Categorical 			| ANOVA						| Chi-Square		|

**2. Wrapper methods**

- **BorutaPy**: [BorutaPy – an all relevant feature selection method](http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/)

**3. Embedded methods**
- [A Complete Tutorial on Ridge and Lasso Regression in Python](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/)

## Parameter tuning

- [Complete Guide to Parameter Tuning in XGBoost (with codes in Python)](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
- [How to Best Tune Multithreading Support for XGBoost in Python](http://machinelearningmastery.com/best-tune-multithreading-support-xgboost-python/)
- **Hyperopt**: [Effectively running thousands of experiments: Hyperopt with Sacred](https://gab41.lab41.org/effectively-running-thousands-of-experiments-hyperopt-with-sacred-dfa53b50f1ec#.8l9i8vc5q)
- **Code** [Automatic model tuning with Sacred and Hyperopt](https://github.com/gereleth/kaggle-telstra/blob/master/Automatic%20model%20tuning%20with%20Sacred%20and%20Hyperopt.ipynb)

## Online Courses

- _Beginner_: [Intro to Inferential Statistics](https://classroom.udacity.com/courses/ud201/lessons/1234788951/concepts/12191788780923) provided by **Udacity**
- _Beginner_: [Intro to Descriptive Statistics](https://classroom.udacity.com/courses/ud827/lessons/1293178557/concepts/19610485690923) provided by **Udacity**
- _Intermediate_: [Intro to Machine Learning](https://classroom.udacity.com/courses/ud120/lessons/2410328539/concepts/24185385370923#) provided by **Udacity**
- _Intermediate_: [Machine Learning](https://www.coursera.org/learn/machine-learning) provided by **Coursera**
- _Advance_: [Deep Learning](https://classroom.udacity.com/courses/ud730/lessons/7320377048/concepts/375715a0-343a-4e9e-b312-405dc5ad79b0) provided by **Udacity**

## Blog

- [Analytics Vidhya](https://www.analyticsvidhya.com/)
- [Machine Learning Mastery](http://machinelearningmastery.com/)

## Youtube Channel
- [sentdex](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ)
- [Dan Van Boxel](https://www.youtube.com/channel/UC6tnRFKGiq1DlybcqP5rZ7A)
- [Siraj Raval](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A)
- [CS231n](https://www.youtube.com/channel/UC2__PIf36huAgKFumlOIs6A)
- [Nando de Freitas](https://www.youtube.com/channel/UC0z_jCi0XWqI8awUuQRFnyw)

## Competitions in Kaggle 

- [Otto Group Product Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge)
- [Shelter Animal Outcomes](https://www.kaggle.com/c/shelter-animal-outcomes)
